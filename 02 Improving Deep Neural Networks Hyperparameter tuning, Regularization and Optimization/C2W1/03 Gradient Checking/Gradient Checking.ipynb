{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Gradient Checking.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPT7P99OR+IIrA5Ig+UzUUQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Uo73sy28xSv-"},"source":["# Gradient Checking\n","\n","I basically followed https://github.com/Kulbear 's work on Andrew Ng's Deep Learning Specialization C2W1 homework assignment with some *modification*"]},{"cell_type":"markdown","metadata":{"id":"VSXFsobK0NQi"},"source":["## 1. Importing libraries and setting up enviornments"]},{"cell_type":"code","metadata":{"id":"54DRhS1XxRJ-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608193920620,"user_tz":-480,"elapsed":496,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}},"outputId":"bc545b5a-937f-4b3f-d83d-664cea53ae24"},"source":["# import public libraries\n","import numpy as np\n","\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","\n","import sys\n","\n","sys.path.append('/content/gdrive/MyDrive/Colab Notebooks/Deep Learning Specialization/02 Improving Deep Neural Networks Hyperparameter tuning, Regularization and Optimization/C2W1/03 Gradient Checking')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oOymyJUZzGgB","executionInfo":{"status":"ok","timestamp":1608193979274,"user_tz":-480,"elapsed":760,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}}},"source":["# import user-defined packages\n","import testCases\n","from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wRZUPCno1zK0"},"source":["## 2. 1-dimensional gradient checking"]},{"cell_type":"code","metadata":{"id":"FBBoSnDg06xU","executionInfo":{"status":"ok","timestamp":1608194260241,"user_tz":-480,"elapsed":921,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}}},"source":["def forward_propagation(x, theta):\n","  \"\"\"\n","  Implement the linear forward propagation (compute J)\n","\n","  Arguments:\n","  x -- a real-valued input\n","  theta -- our parameter, a real number as well\n","\n","  Returns:\n","  J -- the value of function J, computed using the formula J(theta) = theta * x\n","  \"\"\"\n","\n","  return theta * x"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tBIO2Ccf2PlE","executionInfo":{"status":"ok","timestamp":1608194267513,"user_tz":-480,"elapsed":876,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}},"outputId":"881f44d5-0e0e-4378-b9cc-297f2f7e8f22"},"source":["x, theta = 2, 4\n","J = forward_propagation(x, theta)\n","print (\"J = \" + str(J))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["J = 8\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6DDFY_Nu2RXm","executionInfo":{"status":"ok","timestamp":1608194319770,"user_tz":-480,"elapsed":502,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}}},"source":["# GRADED FUNCTION: backward_propagation\n","\n","def backward_propagation(x, theta):\n","  \"\"\"\n","  Computes the derivative of J with respect to theta (see Figure 1).\n","\n","  Arguments:\n","  x -- a real-valued input\n","  theta -- our parameter, a real number as well\n","\n","  Returns:\n","  dtheta -- the gradient of the cost with respect to theta\n","  \"\"\"\n","  dtheta = x\n","  return dtheta"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"One6Jbqu2eMw","executionInfo":{"status":"ok","timestamp":1608194329172,"user_tz":-480,"elapsed":806,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}},"outputId":"7cf5af13-1ffe-474b-f70c-6c46d47653b1"},"source":["x, theta = 2, 4\n","dtheta = backward_propagation(x, theta)\n","print (\"dtheta = \" + str(dtheta))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["dtheta = 2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V5RKOnnc2gcH","executionInfo":{"status":"ok","timestamp":1608195302564,"user_tz":-480,"elapsed":1060,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}}},"source":["def gradient_check(x, theta, epsilon=1e-7):\n","  \"\"\"\n","  Implement the backward propagation presented in Figure 1.\n","\n","  Arguments:\n","  x -- a real-valued input\n","  theta -- our parameter, a real number as well\n","  epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n","\n","  Returns:\n","  difference -- difference (2) between the approximated gradient and the backward propagation gradient\n","  \"\"\"\n","  thetaplus = theta + epsilon\n","  thetaminus = theta - epsilon\n","  J_plus = forward_propagation(x, thetaplus)\n","  J_minus = forward_propagation(x, thetaminus)\n","  gradapprox = ( J_plus  -  J_minus) / (2  *  epsilon)\n","\n","  grads =  backward_propagation(x, theta)\n","\n","  numerator = np.linalg.norm( grads - gradapprox )\n","  denumerator = np.linalg.norm(gradapprox) + np.linalg.norm(grads) \n","  difference =  numerator / denumerator\n","\n","  if difference < 1e-7:\n","    print('Correct')\n","  else:\n","    print('incorrect')\n","\n","  return difference"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GRhddKum5o2C","executionInfo":{"status":"ok","timestamp":1608195304026,"user_tz":-480,"elapsed":778,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}},"outputId":"e38cc863-63fb-4750-9d86-f2f67b1895c2"},"source":["x, theta = 2, 4\n","difference = gradient_check(x, theta)\n","print(\"difference = \" + str(difference))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Correct\n","difference = 2.919335883291695e-10\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ymHh3J4O6Yu3"},"source":["## 3. N-dimensional gradient checking"]},{"cell_type":"code","metadata":{"id":"SeDYi1yU52wF","executionInfo":{"status":"ok","timestamp":1608196766374,"user_tz":-480,"elapsed":1203,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}}},"source":["def forward_propagation_n(X, Y, parameters):\n","  \"\"\"\n","  Implements the forward propagation (and computes the cost) presented in Figure 3.\n","\n","  Arguments:\n","  X -- training set for m examples\n","  Y -- labels for m examples \n","  parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n","                  W1 -- weight matrix of shape (5, 4)\n","                  b1 -- bias vector of shape (5, 1)\n","                  W2 -- weight matrix of shape (3, 5)\n","                  b2 -- bias vector of shape (3, 1)\n","                  W3 -- weight matrix of shape (1, 3)\n","                  b3 -- bias vector of shape (1, 1)\n","\n","  Returns:\n","  cost -- the cost function (logistic cost for one example)\n","  cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n","  \"\"\"\n","  m = X.shape[1]\n","  W1 = parameters['W1']\n","  b1 = parameters['b1']\n","  W2= parameters['W2']\n","  b2 = parameters['b2']\n","  W3 = parameters['W3']\n","  b3 = parameters['b3']\n","\n","  Z1 = np.dot(W1, X) + b1\n","  A1 = relu(Z1)\n","  Z2 = np.dot(W2, A1) + b2\n","  A2 = relu(Z2)\n","  Z3= np.dot(W3, A2) + b3\n","  A3 = sigmoid(Z3)\n","\n","  logprobs = np.multiply(Y, np.log(A3)) + np.multiply(1-Y, np.log(1-A3))\n","  cost = ( -1 / m) * np.sum(logprobs)\n","\n","  cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n","    \n","  return cost, cache\n"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"y3rVlb-h8WdB","executionInfo":{"status":"ok","timestamp":1608196768214,"user_tz":-480,"elapsed":849,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}}},"source":["def backward_propagation_n(X, Y, cache):\n","  \"\"\"\n","  Implement the backward propagation presented in figure 2.\n","\n","  Arguments:\n","  X -- input datapoint, of shape (input size, 1)\n","  Y -- true \"label\"\n","  cache -- cache output from forward_propagation_n()\n","\n","  Returns:\n","  gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n","  \"\"\"\n","\n","  m = X.shape[1]\n","  (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n","\n","  dZ3 = A3 - Y\n","  dW3 = 1. / m * np.dot(dZ3, A2.T)\n","  db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n","\n","  dA2 = np.dot(W3.T, dZ3)\n","  dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n","  dW2 = 1. / m * np.dot(dZ2, A1.T) * 2  # Should not multiply by 2\n","  db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n","\n","  dA1 = np.dot(W2.T, dZ2)\n","  dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n","  dW1 = 1. / m * np.dot(dZ1, X.T)\n","  db1 = 4. / m * np.sum(dZ1, axis=1, keepdims=True) # Should not multiply by 4\n","\n","  gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n","                \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n","                \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n","\n","  return gradients"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"LOvbGQx09Bhs","executionInfo":{"status":"ok","timestamp":1608196769213,"user_tz":-480,"elapsed":563,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}}},"source":["# GRADED FUNCTION: gradient_check_n\n","\n","def gradient_check_n(parameters, gradients, X, Y, epsilon=1e-7):\n","  \"\"\"\n","  Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n","\n","  Arguments:\n","  parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n","  grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n","  x -- input datapoint, of shape (input size, 1)\n","  y -- true \"label\"\n","  epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n","\n","  Returns:\n","  difference -- difference (2) between the approximated gradient and the backward propagation gradient\n","  \"\"\"\n","  parameters_values, _ = dictionary_to_vector(parameters)\n","  grad = gradients_to_vector(gradients)\n","  num_parameters = parameters_values.shape[0]\n","  J_plus = np.zeros((num_parameters, 1))\n","  J_minus = np.zeros((num_parameters, 1))\n","  gradapprox = np.zeros((num_parameters, 1))\n","\n","  for i in range(num_parameters):\n","      # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n","      # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n","      ### START CODE HERE ### (approx. 3 lines)\n","      thetaplus =  np.copy(parameters_values)                                       # Step 1\n","      thetaplus[i][0] = thetaplus[i][0] + epsilon                                   # Step 2\n","      J_plus[i], _ =  forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))  # Step 3\n","      ### END CODE HERE ###\n","      \n","      # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n","      ### START CODE HERE ### (approx. 3 lines)\n","      thetaminus = np.copy(parameters_values)                                       # Step 1\n","      thetaminus[i][0] = thetaminus[i][0] - epsilon                                 # Step 2        \n","      J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus)) # Step 3\n","      ### END CODE HERE ###\n","      \n","      # Compute gradapprox[i]\n","      ### START CODE HERE ### (approx. 1 line)\n","      gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n","      ### END CODE HERE ###\n","\n","  # Compare gradapprox to backward propagation gradients by computing difference.\n","  ### START CODE HERE ### (approx. 1 line)\n","  numerator = np.linalg.norm(grad - gradapprox)                                     # Step 1'\n","  denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                   # Step 2'\n","  difference = numerator / denominator                                              # Step 3'\n","  ### END CODE HERE ###\n","\n","  if difference > 1e-7:\n","      print(\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n","  else:\n","      print(\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n","\n","  return difference"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Q-ynf27-o6V","executionInfo":{"status":"ok","timestamp":1608196770777,"user_tz":-480,"elapsed":931,"user":{"displayName":"施永欣","photoUrl":"","userId":"13262877295823916063"}},"outputId":"5bd3ed84-8741-469a-a2a2-f0d98e90094b"},"source":["X, Y, parameters = testCases.gradient_check_n_test_case()\n","\n","cost, cache = forward_propagation_n(X, Y, parameters)\n","gradients = backward_propagation_n(X, Y, cache)\n","difference = gradient_check_n(parameters, gradients, X, Y)"],"execution_count":41,"outputs":[{"output_type":"stream","text":["\u001b[93mThere is a mistake in the backward propagation! difference = 0.2850931567761624\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YJIsyHsF-rVM"},"source":[""],"execution_count":null,"outputs":[]}]}